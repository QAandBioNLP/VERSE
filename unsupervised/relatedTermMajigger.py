import sys
import fileinput
import argparse
import time
from collections import Counter
import itertools
from textExtractionUtils import *

from java.util import *
from edu.stanford.nlp.pipeline import *
from edu.stanford.nlp.ling.CoreAnnotations import *
from edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations import *

# Given a tokenized bit of text, find all the words that
# are in a lookup dictionary. Find longest terms first.
def getTermIDsAndTypesWithLocations(np, lookupDict):
	termIDsTypesAndLocs = []
	
	# Lowercase all the tokens
	np = [ unicodeLower(w) for w in np ]

	# The length of each search string will decrease from the full length
	# of the text down to 1
	for l in reversed(range(1, len(np)+1)):
		# We move the search window through the text
		for i in range(len(np)-l+1):
			# Extract that window of text
			s = tuple(np[i:i+l])
			# Search for it in the dictionary
			if s in lookupDict:
				# If found, save the ID(s) in the dictionary
				id,thisType = lookupDict[s]
				termIDsTypesAndLocs.append((i,i+l-1,id,thisType))
				# And blank it out
				np[i:i+l] = [ "" for _ in range(l) ]
	
	# Then return the found term IDs
	return termIDsTypesAndLocs

def matchTokenLocations(tokens, sentence):
	prevPos = 0
	locations = []
	for t in tokens:
		#print t, sentence
		pos = sentence.find(t,prevPos)
		if pos == -1:
			return None
		end = pos+len(t)-1
		locations.append((pos,pos+len(t)-1))
		prevPos	= end+1
	return locations
	
triggerCount = 0
eventCount = 0
txtOffset = 0
relations = set()

# Find all co-occurrences for a list of text, or just a line of text
def identifyTerms(outFile, textInput, textSourceInfo):
	global triggerCount
	global eventCount
	global txtOffset
	global relations
	
	pipeline = getPipeline()

	# First check if it is a list of text or just text
	if isinstance(textInput, list):
		textList = textInput
	else:
		textList = [textInput]
	
	# Go through each bit of text
	for text in textList:
		# Remove weird text issues
		text = handleEncoding(text)
		
		document = pipeline.process(text)
		
		# Extract each sentence
		for sentence in document.get(SentencesAnnotation):
			
			# Tokenize each sentence
			tokenObjs = sentence.get(TokensAnnotation)
			tokens = [ t.word().encode('utf8') for t in tokenObjs ]
			
			# Get the IDs of terms found in the sentence
			idsTypesAndLocations = getTermIDsAndTypesWithLocations(tokens, idAndTypeLookup)
			
			#if len(idsTypesAndLocations) > 0:
			#	print "-------------------------"
			#	print sentence
			#	print tokens
			#	print [ unicodeLower(t) for t in tokens ]
			#	print idsTypesAndLocations
			#	print "-------------------------"
			#	print

			knownRelation = False
			for (_,_,id1,_),(_,_,id2,_) in itertools.combinations(idsTypesAndLocations,2):
				if (id1,id2) in relations:
					knownRelation = True
					break

			if knownRelation:
				tokenLocs = matchTokenLocations(tokens, sentence)
				
				if not tokenLocs is None:
					for (start,end,id,thisType) in idsTypesAndLocations:
						charstart,charend = tokenLocs[start][0],tokenLocs[end][1]
						triggerOut = "T%d\t%s %d %d\t%s" % (triggerCount+1,thisType,txtOffset+charstart,txtOffset+charend+1,sentence[charstart:charend+1])
						args.outA1File.write(triggerOut + "\n")
						triggerCount = triggerCount + 1
						
					sentenceOut = "%s \n" % sentence
					args.outTxtFile.write(sentenceOut)
					txtOffset = txtOffset + len(sentenceOut)
		
# It's the main bit. Yay!
if __name__ == "__main__":

	# Arguments for the command line
	parser = argparse.ArgumentParser(description='Full co-occurrence pipeline that extracts text from MEDLINE/PMC files, splits into sentences using LingPipe, tags with GENIA and does more!')
	parser.add_argument('--termsWithSynonymsFile', type=argparse.FileType('r'), help='A list of terms for extraction with synonyms separated by a |')
	parser.add_argument('--binaryTermsFile', type=argparse.FileType('rb'), help='A pickled representation of a word-list, previously generated by this script')
	parser.add_argument('--binaryTermsFile_out', type=argparse.FileType('wb'), help='An output file to save the term list to in a binary format (for faster loading later)')

	parser.add_argument('--relationsFile', type=argparse.FileType('r'), help='')
	
	parser.add_argument('--stopwordsFile',  type=argparse.FileType('r'), help='A path to a stopwords file that will be removed from term-lists (e.g. the, there)')
	parser.add_argument('--removeShortwords', help='Remove short words from any term lists (<=2 length)', action='store_true')

	parser.add_argument('--abstractsFile', type=argparse.FileType('r'), help='MEDLINE file containing abstract data')
	parser.add_argument('--articleFile', type=argparse.FileType('r'), help='PMC NXML file containing a single article')
	parser.add_argument('--articleFilelist', type=argparse.FileType('r'), help='File containing filenames of multiple PMC NXML files')

	parser.add_argument('--outTxtFile', type=argparse.FileType('w'), help='')
	parser.add_argument('--outA1File', type=argparse.FileType('w'), help='')

	args = parser.parse_args()
	
	# Output execution information
	print "####################"
	print " pairedTermMajigger "
	print "####################"
	print ""
	if args.termsWithSynonymsFile:
		print "--termsWithSynonymsFile", args.termsWithSynonymsFile.name
	if args.binaryTermsFile:
		print "--binaryTermsFile", args.binaryTermsFile.name
	if args.binaryTermsFile_out:
		print "--binaryTermsFile_out", args.binaryTermsFile_out.name
	if args.relationsFile:
		print "--relationsFile", args.relationsFile.name
	if args.stopwordsFile:
		print "--stopwordsFile", args.stopwordsFile.name
	if args.removeShortwords:
		print "--removeShortwords"
	if args.abstractsFile:
		print "--abstractsFile", args.abstractsFile.name
	if args.articleFile:
		print "--articleFile", args.articleFile.name
	if args.articleFilelist:
		print "--articleFilelist", args.articleFilelist.name
	if args.outTxtFile:
		print "--outTxtFile", args.outTxtFile.name
	if args.outA1File:
		print "--outA1File", args.outA1File.name
	print ""
	print "######################"
	print ""

	# If loading terms from a text file, close the handle so that it be re-opened
	# with a Unicode codec later
	if args.termsWithSynonymsFile:
		wordlistPath = args.termsWithSynonymsFile.name
		args.termsWithSynonymsFile.close()
	else:
		wordlistPath = None

	#relations = set()
	for line in args.relationsFile:
		a,b = map(int,line.strip().split('\t'))
		relations.add((a,b))
		relations.add((b,a))
	
	# Dictionary to contain terms->IDs from a word-list
	idAndTypeLookup = loadNoDuplicateWordlistFileWithTypes(wordlistPath, args.stopwordsFile, args.removeShortwords, args.binaryTermsFile, args.binaryTermsFile_out)
		
	startTime = time.time()

	# And now we try to process either an abstract file, single article file or multiple
	# article files
	try:
		if args.abstractsFile:
			processAbstractFile(args.abstractsFile, None, identifyTerms)
		elif args.articleFile:
			# Just pull the filename and pass that, instead of the object
			filename = args.articleFile.name
			args.articleFile.close()
			processArticleFiles(filename, None, identifyTerms)
		elif args.articleFilelist:
			# Extract the file list from another file
			fileList = [ f.strip() for f in args.articleFilelist]
		
			processArticleFiles(fileList, None, identifyTerms)
	except:
		print "Unexpected error:", sys.exc_info()[0]
		print "COMMAND: " + " ".join(sys.argv)
		raise

	endTime = time.time()
	duration = endTime - startTime
	print "Processing Time: ", duration
	
